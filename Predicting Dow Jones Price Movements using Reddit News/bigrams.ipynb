{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "from sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor,LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import random\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import six\n",
    "from abc import ABCMeta\n",
    "from scipy import sparse\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import defaultdict\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import backend as K\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Dow Jones Index Price Movements using bigrams consructed from Reddit News Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we look how combination of two words (so-called bigrams) today affect the dow jones price movement today. We find that logistic regression gives us the best accuracy of 57.14%. We additionally use decision tree classifier and random forest classifiers and get the accuracy of 54%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('/Users/nastyademina/Desktop/r studio/Combined_News_DJIA.csv')\n",
    "train = news[news['Date'] < '2015-01-01']\n",
    "test = news[news['Date'] > '2014-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic Regression 2 accuracy:  0.5714285714285714\n",
      "    0    1\n",
      "0  72  114\n",
      "1  48  144\n",
      "0.75\n",
      "0.6129032258064516\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.97, max_features = 200000, ngram_range = (2, 2))\n",
    "\n",
    "trainheadlines = []\n",
    "for row in range(0,len(train.index)):\n",
    "    trainheadlines.append(' '.join(str(x) for x in train.iloc[row,2:27]))\n",
    "\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "advancedmodel = LogisticRegression()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds_lr = advancedmodel.predict(advancedtest)\n",
    "acc_lr=accuracy_score(test['Label'], preds_lr)\n",
    "\n",
    "print('Logic Regression 2 accuracy: ', acc_lr)\n",
    "\n",
    "results = pd.DataFrame(confusion_matrix(test['Label'], preds_lr))\n",
    "print(results)\n",
    "TPR=144/(48+144)\n",
    "print(TPR)\n",
    "FPR=114/(72+114)\n",
    "print(FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>the islamic</td>\n",
       "      <td>0.238449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>the most</td>\n",
       "      <td>0.236327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>the euro</td>\n",
       "      <td>0.232905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>as an</td>\n",
       "      <td>0.231748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>court has</td>\n",
       "      <td>0.231301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>of being</td>\n",
       "      <td>0.230875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>wikileaks founder</td>\n",
       "      <td>0.230030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>they have</td>\n",
       "      <td>0.228450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>based on</td>\n",
       "      <td>0.227673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>last year</td>\n",
       "      <td>0.226998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>jailed for</td>\n",
       "      <td>0.222206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>of their</td>\n",
       "      <td>0.221569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>thousands of</td>\n",
       "      <td>0.220385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>has become</td>\n",
       "      <td>0.218388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>jong un</td>\n",
       "      <td>0.218334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>for their</td>\n",
       "      <td>0.216048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>david cameron</td>\n",
       "      <td>0.212341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>war in</td>\n",
       "      <td>0.201552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>want to</td>\n",
       "      <td>0.201452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>refusing to</td>\n",
       "      <td>0.201212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>was found</td>\n",
       "      <td>0.200129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>about the</td>\n",
       "      <td>0.189191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>she was</td>\n",
       "      <td>0.185530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>which is</td>\n",
       "      <td>0.183047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>law that</td>\n",
       "      <td>0.181995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>efforts to</td>\n",
       "      <td>0.180366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>were killed</td>\n",
       "      <td>0.178340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>in iraq</td>\n",
       "      <td>0.176267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>to give</td>\n",
       "      <td>0.175498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>in north</td>\n",
       "      <td>0.174494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>head of</td>\n",
       "      <td>0.112452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>now the</td>\n",
       "      <td>0.111879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>warns of</td>\n",
       "      <td>0.110344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>the government</td>\n",
       "      <td>0.110094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>attacks on</td>\n",
       "      <td>0.109713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>in eastern</td>\n",
       "      <td>0.109285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>that it</td>\n",
       "      <td>0.108846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>deal with</td>\n",
       "      <td>0.107702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>years of</td>\n",
       "      <td>0.105071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>edward snowden</td>\n",
       "      <td>0.105051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>in germany</td>\n",
       "      <td>0.103685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>control of</td>\n",
       "      <td>0.101244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>to remove</td>\n",
       "      <td>0.099884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>kim jong</td>\n",
       "      <td>0.099101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>involved in</td>\n",
       "      <td>0.098658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>to prevent</td>\n",
       "      <td>0.096563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>the head</td>\n",
       "      <td>0.095650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>when the</td>\n",
       "      <td>0.089276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>of world</td>\n",
       "      <td>0.088024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>behind the</td>\n",
       "      <td>0.087502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>al qaida</td>\n",
       "      <td>0.086064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>in afghanistan</td>\n",
       "      <td>0.086056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>due to</td>\n",
       "      <td>0.085703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>to bring</td>\n",
       "      <td>0.085681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>sent to</td>\n",
       "      <td>0.080174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>not be</td>\n",
       "      <td>0.079515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>group of</td>\n",
       "      <td>0.077880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>tony blair</td>\n",
       "      <td>0.077140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>day of</td>\n",
       "      <td>0.071665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>city of</td>\n",
       "      <td>0.067654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Words  Coefficient\n",
       "471        the islamic     0.238449\n",
       "480           the most     0.236327\n",
       "457           the euro     0.232905\n",
       "43               as an     0.231748\n",
       "100          court has     0.231301\n",
       "313           of being     0.230875\n",
       "635  wikileaks founder     0.230030\n",
       "520          they have     0.228450\n",
       "61            based on     0.227673\n",
       "273          last year     0.226998\n",
       "263         jailed for     0.222206\n",
       "327           of their     0.221569\n",
       "528       thousands of     0.220385\n",
       "162         has become     0.218388\n",
       "265            jong un     0.218334\n",
       "140          for their     0.216048\n",
       "104      david cameron     0.212341\n",
       "611             war in     0.201552\n",
       "608            want to     0.201452\n",
       "386        refusing to     0.201212\n",
       "614          was found     0.200129\n",
       "7            about the     0.189191\n",
       "413            she was     0.185530\n",
       "629           which is     0.183047\n",
       "274           law that     0.181995\n",
       "122         efforts to     0.180366\n",
       "622        were killed     0.178340\n",
       "206            in iraq     0.176267\n",
       "553            to give     0.175498\n",
       "216           in north     0.174494\n",
       "..                 ...          ...\n",
       "172            head of     0.112452\n",
       "306            now the     0.111879\n",
       "613           warns of     0.110344\n",
       "465     the government     0.110094\n",
       "53          attacks on     0.109713\n",
       "195         in eastern     0.109285\n",
       "437            that it     0.108846\n",
       "107          deal with     0.107702\n",
       "655           years of     0.105071\n",
       "121     edward snowden     0.105051\n",
       "201         in germany     0.103685\n",
       "96          control of     0.101244\n",
       "573          to remove     0.099884\n",
       "270           kim jong     0.099101\n",
       "240        involved in     0.098658\n",
       "570         to prevent     0.096563\n",
       "468           the head     0.095650\n",
       "628           when the     0.089276\n",
       "333           of world     0.088024\n",
       "71          behind the     0.087502\n",
       "20            al qaida     0.086064\n",
       "184     in afghanistan     0.086056\n",
       "119             due to     0.085703\n",
       "541           to bring     0.085681\n",
       "408            sent to     0.080174\n",
       "303             not be     0.079515\n",
       "158           group of     0.077880\n",
       "586         tony blair     0.077140\n",
       "105             day of     0.071665\n",
       "92             city of     0.067654\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advwords = advancedvectorizer.get_feature_names()\n",
    "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
    "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
    "                        'Coefficient' : advcoeffs})\n",
    "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
    "advcoeffdf[200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5396825396825397\n",
      "     0    1\n",
      "0  104   82\n",
      "1   92  100\n",
      "0.53125\n",
      "0.478494623655914\n"
     ]
    }
   ],
   "source": [
    "#decision tree\n",
    "tree = DecisionTreeClassifier()\n",
    "tree= tree.fit(advancedtrain, train[\"Label\"])\n",
    "\n",
    "preds_tree = tree.predict(advancedtest)\n",
    "acc_tree = accuracy_score(test['Label'], preds_tree)\n",
    "print(acc_tree)\n",
    "\n",
    "results = pd.DataFrame(confusion_matrix(test['Label'], preds_tree))\n",
    "print(results)\n",
    "\n",
    "TPR=102/(90+102)\n",
    "print(TPR)\n",
    "FPR=89/(89+97)\n",
    "print(FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5317460317460317\n",
      "     0    1\n",
      "0  104   82\n",
      "1   92  100\n",
      "0.5052083333333334\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "rf = RandomForestClassifier()\n",
    "rf = rf.fit(advancedtrain, train[\"Label\"])\n",
    "\n",
    "preds_rf = rf.predict(advancedtest)\n",
    "acc_rf = accuracy_score(test['Label'], preds_rf)\n",
    "print(acc_rf)\n",
    "\n",
    "results = pd.DataFrame(confusion_matrix(test['Label'], preds_tree))\n",
    "print(results)\n",
    "\n",
    "TPR=97/(97+95)\n",
    "print(TPR)\n",
    "FPR=93/(93+93)\n",
    "print(FPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams analysis with lagged news data and lagged Dow Jones Index price movements added as an independent variables starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this section we will test whether we can improve the prediction power of our model by using bigrams. We use python advanced functions for NLP-TfidfVectorizer to create bigrams. Additionally, we add one more independent variable to capture other effects which is equal to 1 if yesterday Dow Jones index price went up or stayed the same and is equal to 0 if Dow Jones Index went down. \n",
    "* We use lead of the Label variable as our dependent variable. Therefore, we try to predict price movements of Dow Jones tomorrow using news topics today and price movements of Dow Jones today.\n",
    "* We run logistric regression, decision tree classifier and random forest classifier once again.\n",
    "* From our results we can see that our models do not have high accuracy on the test set (Logic Regression 2 accuracy:  0.517, Decision Tree 1 accuracy:  0.509, Random Forest 1 accuracy:  0.516)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(378, 27)\n",
      "(1611, 27)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/nastyademina/Desktop/r studio/Combined_News_DJIA.csv')\n",
    "train = data[data['Date'] < '2015-01-01']\n",
    "test = data[data['Date'] > '2014-12-31']\n",
    "print(test.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating bigrams and running logistic regression\n",
    "#gives us 52% accuracy \n",
    "test['Label_lead'] = test['Label'].shift(-1)\n",
    "test.drop(test.tail(1).index,inplace=True)\n",
    "\n",
    "train['Label_lead'] = train['Label'].shift(-1)\n",
    "train.drop(train.tail(1).index,inplace=True)\n",
    "train=pd.DataFrame(train)\n",
    "\n",
    "trainheadlines = []\n",
    "for row in range(0,len(train.index)):\n",
    "    trainheadlines.append(' '.join(str(x) for x in train.iloc[row,2:27]))\n",
    "\n",
    "#how to add it to the training set\n",
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.97, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "av = pd.DataFrame(advancedtrain.toarray(), columns=advancedvectorizer.get_feature_names())\n",
    "\n",
    "label=pd.DataFrame(train['Label'])\n",
    "advancedtrain= pd.concat([label, av], axis=1)\n",
    "\n",
    "advancedmodel = LogisticRegression()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label_lead\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(377, 657)\n",
      "Logic Regression 2 accuracy:  0.5172413793103449\n"
     ]
    }
   ],
   "source": [
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "\n",
    "\n",
    "av_test = pd.DataFrame(advancedtest.toarray(), columns=advancedvectorizer.get_feature_names())\n",
    "label_test=pd.DataFrame(test.iloc[:,1])\n",
    "\n",
    "av_test.reset_index(drop=True, inplace=True)\n",
    "label_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "advancedtest= pd.concat([label_test, av_test], axis=1)\n",
    "\n",
    "print(advancedtest.shape)\n",
    "\n",
    "preds2 = advancedmodel.predict(advancedtest)\n",
    "acc2=accuracy_score(test['Label_lead'], preds2)\n",
    "print('Logic Regression 2 accuracy: ', acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT 1 accuracy:  0.5092838196286472\n"
     ]
    }
   ],
   "source": [
    "#decision tree\n",
    "advancedmodel = DecisionTreeClassifier(criterion = 'entropy',max_depth = 8, min_samples_leaf = 8, min_impurity_split = 0.5,random_state=123)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label_lead\"])\n",
    "preds_dt = advancedmodel.predict(advancedtest)\n",
    "acc_dt = accuracy_score(test['Label_lead'], preds_dt)\n",
    "print('DT 1 accuracy: ', acc_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0    1\n",
      "0  85  101\n",
      "1  93   98\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(confusion_matrix(test['Label_lead'], preds_dt))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF 1 accuracy:  0.5172413793103449\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "advancedmodel = RandomForestClassifier(\n",
    "    n_estimators = 20,\n",
    "    criterion='entropy', \n",
    "    max_depth=8, \n",
    "    min_samples_leaf=8, \n",
    "    n_jobs=5, \n",
    "    random_state=13451)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label_lead\"])\n",
    "preds_rf = advancedmodel.predict(advancedtest)\n",
    "acc_rf = accuracy_score(test['Label_lead'], preds_rf)\n",
    "print('RF 1 accuracy: ', acc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0    1\n",
      "0  28  158\n",
      "1  24  167\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(confusion_matrix(test['Label_lead'], preds_rf))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our baseline model we have only one independent variable, which is equal to 1 if yesterday Dow Jones index price went up or stayed the same and is equal to 0 if Dow Jones Index went down. We can see that baseline model always predicts 1. Therefore, accuracy of logistic regression, decision tree and random forest models is the same and equals to 0.505."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('/Users/nastyademina/Desktop/r studio/Combined_News_DJIA.csv')\n",
    "train1 = data1[data1['Date'] < '2015-01-01']\n",
    "test1 = data1[data1['Date'] > '2014-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1['Label_lead'] = test1['Label'].shift(-1)\n",
    "test1.drop(test1.tail(1).index,inplace=True)\n",
    "\n",
    "train1['Label_lead'] = train1['Label'].shift(-1)\n",
    "train1.drop(train1.tail(1).index,inplace=True)\n",
    "\n",
    "x_train=pd.DataFrame(train1['Label'])\n",
    "y_train=pd.DataFrame(train1['Label_lead'])\n",
    "\n",
    "x_test=pd.DataFrame(test1['Label'])\n",
    "y_test=pd.DataFrame(test1['Label_lead'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5053191489361702\n",
      "   0    1\n",
      "0  0  186\n",
      "1  0  190\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "print(accuracy_score(y_test, lr.predict(x_test)))\n",
    "results = pd.DataFrame(confusion_matrix(y_test, lr.predict(x_test)))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1\n",
      "0  0  186\n",
      "1  0  190\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(criterion = 'entropy',max_depth = 8, min_samples_leaf = 8, min_impurity_split = 0.5, random_state=53)\n",
    "tree.fit(x_train, y_train)\n",
    "pred = tree.predict(x_test)\n",
    "accuracy_score(pred, y_test)\n",
    "\n",
    "results = pd.DataFrame(confusion_matrix(y_test, pred))\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1\n",
      "0  0  186\n",
      "1  0  190\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=20, \n",
    "    max_depth=8, \n",
    "    min_samples_leaf=8, \n",
    "    n_jobs=5, \n",
    "    random_state=123\n",
    ")\n",
    "rf.fit(x_train, y_train)\n",
    "accuracy_score(y_test, rf.predict(x_test))\n",
    "\n",
    "res = pd.DataFrame(confusion_matrix(x_test, rf.predict(x_test)))\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
